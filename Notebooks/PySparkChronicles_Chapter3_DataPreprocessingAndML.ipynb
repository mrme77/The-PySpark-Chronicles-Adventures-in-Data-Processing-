{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0c1f73-35b0-43be-a834-c5bdf70ff871",
   "metadata": {},
   "source": [
    "### Libraries and data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2a282c-36ea-4462-be3b-d6d9abf946ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_libraries import * \n",
    "import project_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57124fb-1767-40c0-a71f-cef052b827cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/20 19:58:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.debug.catalog\", False) \\\n",
    "    .config(\"spark.logLevel\", \"ERROR\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf5fa7-5c86-4849-ab6e-b9157e0758c5",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f54111-fe6c-4790-827c-1b7dd98ac5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "curated_df = spark.read.parquet(\"curated_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7513a-b84d-4fa9-be13-29c11563a4f5",
   "metadata": {},
   "source": [
    "#### Binary Classification: Grouping related crime categories into broader categories (e.g., robbery crimes vs. non-robbery crimes) and train separate binary classifiers for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b920734-2089-4d74-a477-0cbe072faa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_df = curated_df.withColumn('robbery_crime_type', when(col('crm_cd_desc') == 'ROBBERY', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef06258-6ffd-42aa-8bb4-6514afc8c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_df = curated_df.drop('crm_cd_desc','crm_cd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb14f06e-e7f7-43d9-98a6-849920526f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curated_df.select(col('crm_cd_desc')).distinct().show(40,truncate=False)\n",
    "#curated_df.groupBy('crm_cd_desc').count().orderBy(col('count').desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d8630-2fc3-4e01-8789-6b2ea566c5d9",
   "metadata": {},
   "source": [
    "#### Assessing the balance of the class instances (one class is significantly more prevalent than the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015d186b-744e-4df2-8f2c-288a38a18fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curated_df.toPandas().stalking_crime_type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7207d606-e902-4bdb-9633-d3b20fa86c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+\n",
      "|robbery_crime_type| count|          proportion|\n",
      "+------------------+------+--------------------+\n",
      "|                 1| 31521|0.034050252776217434|\n",
      "|                 0|894199|  0.9659497472237826|\n",
      "+------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "robbery_type_counts = curated_df.groupBy('robbery_crime_type').count()\n",
    "\n",
    "# Calculate the proportion of each crime type\n",
    "proportions = robbery_type_counts.withColumn('proportion', F.col('count') / curated_df.count())\n",
    "\n",
    "proportions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79e75c-c3c6-431a-bf94-a75d4f5fab5d",
   "metadata": {},
   "source": [
    "#### Assessing categorical feature vict_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94da593c-a5dd-4ab6-9d41-82408f41447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|vict_sex|\n",
      "+--------+\n",
      "|       F|\n",
      "| unknown|\n",
      "|       M|\n",
      "|       X|\n",
      "|       H|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "curated_df.select('vict_sex').distinct().show()\n",
    "#count().orderBy('count', ascending=False).first()['vict_sex']\n",
    "#df.groupBy('vict_descent').count().orderBy('count', ascending=False).first()['vict_descent']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3f7be-7130-4f12-994d-996d932a5804",
   "metadata": {},
   "source": [
    "#### Convert categorical features with one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a288efda-a219-4c21-852f-02f74ac04521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "# StringIndexer to convert the 'vict_sex' column into numerical indices\n",
    "stringIndexer = StringIndexer(inputCol=\"vict_sex\", outputCol=\"vict_sex_type_indexed\")\n",
    "\n",
    "# OneHotEncoder to encode the numerical indices into one-hot encoded vectors\n",
    "encoder = OneHotEncoder(inputCol=\"vict_sex_type_indexed\", outputCol=\"vict_sex_type_encoded\")\n",
    "\n",
    "# Define a pipeline that includes both StringIndexer and OneHotEncoder\n",
    "pipeline = Pipeline(stages=[stringIndexer, encoder])\n",
    "\n",
    "# Fit the pipeline to the DataFrame and transform the DataFrame\n",
    "curated_df_encoded = pipeline.fit(curated_df).transform(curated_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc75face-5099-4572-a187-67d078893fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_df_encoded = curated_df_encoded.drop('vict_sex')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8a4c3-431f-417f-ac91-11d84e139be2",
   "metadata": {},
   "source": [
    "#### Subsetting datasets with features of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1141c5-232e-4f9b-8567-2ca66a1fd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = curated_df_encoded.select('vict_age','crime_day_occ', \n",
    "                                 'crime_month_occ', 'crime_year_occ', 'crime_day_rptd', \n",
    "                                 'crime_month_rptd', 'crime_year_rptd', \n",
    "                                 'robbery_crime_type','vict_sex_type_indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b7032a-d472-4c8c-93cc-a7cfe81dc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_features = [curated_df.dtypes[value][0] for value in range(0,len(curated_df.columns)) if curated_df.dtypes[value][1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa970ce-b144-4499-80d8-187f0b321c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical_features = [curated_df.dtypes[value][0] for value in range(0,len(curated_df.columns)) if curated_df.dtypes[value][1]=='int' or curated_df.dtypes[value][1]=='double']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb8028-c894-4304-83b4-8f76755dbcab",
   "metadata": {},
   "source": [
    "### Balance Class Distribution using SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aec9b8-4ac5-4b4d-8909-be9e808c8333",
   "metadata": {},
   "source": [
    "#### SMOTE (Synthetic Minority Over-sampling Technique) is a technique used to balance class distributions by generating synthetic samples of the minority class. It works by creating new instances that are similar to existing minority class instances. This helps address imbalances in the dataset and improves the performance of machine learning models, especially those sensitive to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d5ec94-3ac6-4320-ab75-2c76a3d002f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "train_data_pd = data_model.toPandas()\n",
    "\n",
    "# Apply oversampling \n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(train_data_pd.drop('robbery_crime_type', axis=1), train_data_pd['robbery_crime_type'])\n",
    "\n",
    "# Combine resampled features and target variable\n",
    "resampled_df = pd.DataFrame(X_resampled, columns=train_data_pd.drop('robbery_crime_type', axis=1).columns)\n",
    "resampled_df['robbery_crime_type'] = y_resampled\n",
    "\n",
    "# Convert back to PySpark DataFrame\n",
    "train_data_balanced = spark.createDataFrame(resampled_df).repartition(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e3d4e-9746-4481-8bf6-847acf7e38f9",
   "metadata": {},
   "source": [
    "#### Creating a Vector Assembler which merges multiple columns into a single vector column. It's commonly used to assemble feature vectors for machine learning models in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0929dccf-e166-4067-b292-5c250ed19c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature vector assembler\n",
    "assembler = VectorAssembler(inputCols=['vict_age', 'crime_day_occ', 'crime_month_occ', 'crime_year_occ', \n",
    "                     \n",
    "                                       'crime_day_rptd', 'crime_month_rptd', 'crime_year_rptd', \n",
    "                                       'vict_sex_type_indexed'], outputCol='features')\n",
    "\n",
    "# Assemble features\n",
    "data_assembled = assembler.transform(train_data_balanced)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = data_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a3264-c2f6-489a-9bca-44bc2dcb13b0",
   "metadata": {},
   "source": [
    "#### Now that the data preprocessing is complete, we fit the data into a logistic regression model to classify crimes as either robbery or non-robbery based on various features, evaluates its performance, and prints the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8962719-84d1-4d24-9605-0543f04b0d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.5526471460967626\n"
     ]
    }
   ],
   "source": [
    "# Define Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='scaled_features', labelCol='robbery_crime_type')\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[scaler, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='robbery_crime_type')\n",
    "auc_roc= evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f98ed3d-172f-41ab-8386-cf5781638a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 113:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 83613.  95252.]\n",
      " [ 68628. 110114.]]\n",
      "Precision: 0.5361841784910842\n",
      "Recall: 0.6160499490886305\n",
      "F1 Score: 0.5733491622147937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert DataFrame to an RDD of (prediction, label) tuples\n",
    "predictionAndLabels = predictions.select('prediction', 'robbery_crime_type').rdd.map(lambda row: (float(row['prediction']), float(row['robbery_crime_type'])))\n",
    "\n",
    "# Initialize MulticlassMetrics\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Get the Confusion Matrix\n",
    "confusionMatrix = metrics.confusionMatrix().toArray()\n",
    "print(\"Confusion Matrix:\\n\", confusionMatrix)\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1Score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8baa17-cc97-4569-8a13-7ac97b7d961e",
   "metadata": {},
   "source": [
    "#### Let's interpret each of these metrics:\n",
    "\n",
    "### AUC-ROC: 0.55264631211408\n",
    "- AUC-ROC (Area Under the Receiver Operating Characteristic curve)** measures the ability of a classifier to distinguish between classes and is used as a summary of the model's performance. The ROC curve is a graphical representation of the trade-off between the true positive rate (TPR, recall) and the false positive rate (FPR) across different thresholds.\n",
    "- An AUC-ROC value of 0.5526 is slightly better than random guessing, which has an AUC-ROC of 0.5. However, it's still considered not very good, indicating that the model does not do a great job at distinguishing between the positive and negative classes. Typically, an AUC-ROC closer to 1.0 indicates excellent model performance, while closer to 0.5 suggests no discriminative ability.\n",
    "\n",
    "### Confusion Matrix\n",
    "- The confusion matrix is a 2x2 table that shows the counts of correct and incorrect predictions classified by the actual classes:\n",
    "  ```\n",
    "  [ True Negatives  (TN) | False Positives (FP) ]\n",
    "  [ False Negatives (FN) | True Positives  (TP) ]\n",
    "  ```\n",
    "- In the confusion matrix:\n",
    "  - **True Negatives (TN)**: 83,613 - The number of negative instances correctly classified as negative.\n",
    "  - **False Positives (FP)**: 95,252 - The number of negative instances incorrectly classified as positive.\n",
    "  - **False Negatives (FN)**: 68,628 - The number of positive instances incorrectly classified as negative.\n",
    "  - **True Positives (TP)**: 110,114 - The number of positive instances correctly classified as positive.\n",
    "\n",
    "### Precision: 0.5361841784910842\n",
    "- Precision measures the accuracy of positive predictions. It is defined as the ratio of true positives to the sum of true and false positives.\n",
    "- A precision of 0.5362 indicates that approximately 53.62% of the model’s positive classifications are correct, suggesting that when the model predicts an instance as positive, it is correct about half the time.\n",
    "\n",
    "### Recall: 0.6160499490886305\n",
    "- Recall (or Sensitivity or True Positive Rate) measures the ability of a model to find all the relevant cases (all true positives).\n",
    "- A recall of 0.6160 means that the model correctly identifies about 61.60% of the actual positive cases. Thus, the model is moderately effective at capturing positive instances but still misses around 38.40% of them.\n",
    "\n",
    "### F1 Score: 0.5733491622147937\n",
    "- F1 Score is the harmonic mean of precision and recall. It is a balance between precision and recall, providing a single score that balances both the false positives and false negatives.\n",
    "- An F1 score of 0.5733 suggests a moderate balance between precision and recall, which is not particularly high, indicating that the model is not very effective in terms of precision-recall trade-off.\n",
    "\n",
    "### Overall Interpretation\n",
    "- The model shows limited effectiveness in discriminating between the positive and negative classes as indicated by the AUC-ROC score.\n",
    "- Although it has a moderate recall, its precision is also moderate, leading to a moderate F1 score. This could be indicative of a need to revisit feature selection, model choice, or threshold settings.\n",
    "- The high number of false positives and false negatives suggests potential issues with the model’s ability to generalize or possibly imbalanced class distribution in the data. Consider exploring model improvements or trying different classification algorithms.\n",
    "- Possibly adopt specific data preprocessing, feature engineering, or trying different threshold settings for classification decisions to potentially improve these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9330be-abb9-4ead-9c9a-0beb07546f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

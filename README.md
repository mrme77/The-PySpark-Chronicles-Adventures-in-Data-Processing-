# The PySpark Chronicles: Adventures in Data Processing

![Data Exploration](castle.jpeg)
<br>
*Picture created with Hugging Face Stable Diffusion*
[Huggingface](https://huggingface.co/spaces/stabilityai/stable-diffusion)
<br>
# Exploring the Enchanted World of PySpark

Welcome to the "Exploring the World of PySpark" project! In this project, we delve into the powerful world of PySpark, a Python API for Apache Spark, to perform various data exploration and transformation tasks.

## Overview

Apache Spark is a fast and general-purpose cluster computing system that provides high-level APIs in Java, Scala, and Python, and an optimized engine that supports general execution graphs. PySpark, the Python API for Spark, allows us to interact with Spark using Python.

This project aims to provide a beginner-friendly introduction to PySpark, focusing on basic commands for data exploration and transformations.

## Contents

- **data_exploration.ipynb**: Jupyter Notebook containing basic commands for data exploration using PySpark.
- **data_transformations.ipynb**: Jupyter Notebook demonstrating various data transformation operations in PySpark.
- **requirements.txt**: File listing the Python dependencies required to run the project.

## Getting Started

To run the code in this project, follow these steps:

1. Ensure you have Python and Jupyter Notebook installed on your system.
2. Clone this repository to your local machine.
3. Install the required Python dependencies using `pip install -r requirements.txt`.
4. Open the Jupyter Notebooks `data_exploration.ipynb` and `data_transformations.ipynb` to explore and run the code.

## Dependencies

The project relies on the following Python packages:

- pyspark
- matplotlib
- pandas

You can install these dependencies by running `pip install -r requirements.txt`.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
